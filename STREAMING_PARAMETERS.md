# Guide des param√®tres de streaming avanc√©s

## Vue d'ensemble

Le syst√®me de streaming de Chatterbox inclut maintenant des param√®tres configurables pour contr√¥ler :
- **La d√©tection de r√©p√©tition de tokens** : Arr√™te la g√©n√©ration si trop de tokens identiques cons√©cutifs
- **La d√©tection d'hallucination audio** : Arr√™te si l'audio continue apr√®s la fin du texte
- **Les param√®tres d'√©chantillonnage** : `repetition_penalty`, `min_p`, `top_p`

## Probl√®mes r√©solus

### ‚ùå Probl√®me 1 : G√©n√©ration arr√™t√©e trop t√¥t
**Sympt√¥me** : Le mod√®le d√©tecte des "r√©p√©titions" et arr√™te la g√©n√©ration pr√©matur√©ment

**Cause** : Le seuil de d√©tection de r√©p√©tition de tokens √©tait trop agressif (3 tokens identiques cons√©cutifs)

**Solution** :
```python
model.generate_stream(
    text,
    token_repetition_threshold=8,  # Plus tol√©rant (d√©faut: 5)
    # OU compl√®tement d√©sactiver :
    token_repetition_threshold=0,  # D√©sactiv√©
)
```

### ‚ùå Probl√®me 2 : Audio d'hallucination apr√®s la fin du texte
**Sympt√¥me** : L'audio continue de se g√©n√©rer alors que le texte est termin√©

**Cause** : Les seuils de d√©tection d'hallucination √©taient trop courts

**Solution** :
```python
model.generate_stream(
    text,
    long_tail_threshold=3,  # Plus agressif pour couper rapidement (d√©faut: 5)
    alignment_repetition_threshold=3,
    excessive_tail_threshold=5,  # Arr√™t forc√© plus rapide
)
```

## Param√®tres disponibles dans `generate_stream()`

### Param√®tres d'√©chantillonnage

#### `repetition_penalty` (float, d√©faut: 1.2)
P√©nalit√© pour la r√©p√©tition de tokens lors de l'√©chantillonnage.
- **1.0** : Aucune p√©nalit√©
- **1.2-1.5** : P√©nalit√© mod√©r√©e (recommand√©)
- **2.0+** : P√©nalit√© forte (peut produire du texte moins naturel)

```python
# Exemple : forte p√©nalit√© contre la r√©p√©tition
model.generate_stream(text, repetition_penalty=1.8)
```

#### `min_p` (float, d√©faut: 0.0)
Seuil de probabilit√© minimum pour l'√©chantillonnage.
- **0.0** : D√©sactiv√© (tous les tokens sont consid√©r√©s)
- **0.05-0.1** : Filtre les tokens tr√®s improbables
- Plus la valeur est √©lev√©e, plus l'√©chantillonnage est conservateur

```python
# Exemple : filtrer les tokens peu probables
model.generate_stream(text, min_p=0.05)
```

#### `top_p` (float, d√©faut: 0.95)
√âchantillonnage nucleus (top-p).
- **1.0** : D√©sactiv√© (tous les tokens sont consid√©r√©s)
- **0.9-0.95** : Recommand√© pour la plupart des cas
- **0.8 ou moins** : Plus d√©terministe, moins de variation

```python
# Exemple : √©chantillonnage plus serr√©
model.generate_stream(text, top_p=0.85)
```

### Param√®tres de d√©tection d'hallucination

#### `token_repetition_threshold` (int, d√©faut: 5)
Nombre de tokens identiques cons√©cutifs avant arr√™t forc√©.
- **0** : D√©sactiv√© (permet toute r√©p√©tition)
- **3-5** : Agressif (d√©tecte rapidement)
- **8-10** : Plus tol√©rant

```python
# D√©sactiver compl√®tement la d√©tection de r√©p√©tition de tokens
model.generate_stream(text, token_repetition_threshold=0)

# Plus tol√©rant (permet plus de r√©p√©titions)
model.generate_stream(text, token_repetition_threshold=8)
```

#### `long_tail_threshold` (int, d√©faut: 5)
Nombre de frames d'activation du token final avant arr√™t.
- **0** : D√©sactiv√©
- **3** : Agressif (coupe rapidement)
- **5** : √âquilibr√© (d√©faut)
- **8+** : Tol√©rant (permet des fins plus longues)

```python
# Couper rapidement pour √©viter les hallucinations
model.generate_stream(text, long_tail_threshold=3)
```

#### `alignment_repetition_threshold` (int, d√©faut: 5)
Seuil de r√©activation des tokens pr√©c√©dents apr√®s compl√©tion.
- **0** : D√©sactiv√©
- **3** : Agressif
- **5** : √âquilibr√© (d√©faut)
- **8+** : Tol√©rant

#### `excessive_tail_threshold` (int, d√©faut: 10)
Arr√™t forc√© apr√®s N frames au-del√† de la compl√©tion du texte.
- **0** : D√©sactiv√© (pas d'arr√™t forc√©)
- **5** : Agressif
- **10** : √âquilibr√© (d√©faut)
- **15+** : Tol√©rant

## Cas d'usage et recettes

### üéØ Cas 1 : La g√©n√©ration s'arr√™te trop t√¥t

**Sympt√¥mes** :
- L'audio est coup√© avant la fin de la phrase
- Le mod√®le d√©tecte des "r√©p√©titions" qui n'en sont pas

**Solution** :
```python
for audio_chunk, metrics in model.generate_stream(
    text,
    token_repetition_threshold=10,  # Plus tol√©rant
    long_tail_threshold=8,  # Permet des fins plus longues
    alignment_repetition_threshold=8,
    excessive_tail_threshold=15,  # Arr√™t forc√© plus tardif
):
    ...
```

### üéØ Cas 2 : Audio continue apr√®s la fin du texte (hallucination)

**Sympt√¥mes** :
- L'audio continue de se g√©n√©rer alors que le texte est fini
- Sons ou r√©p√©titions en fin d'audio

**Solution** :
```python
for audio_chunk, metrics in model.generate_stream(
    text,
    long_tail_threshold=3,  # Coupe rapidement
    alignment_repetition_threshold=3,
    excessive_tail_threshold=5,  # Arr√™t forc√© rapide
    repetition_penalty=1.8,  # Forte p√©nalit√© contre la r√©p√©tition
):
    ...
```

### üéØ Cas 3 : D√©sactiver TOUTE d√©tection (exp√©rimental)

**‚ö†Ô∏è Attention** : Peut produire des hallucinations

```python
for audio_chunk, metrics in model.generate_stream(
    text,
    token_repetition_threshold=0,  # D√©sactiv√©
    long_tail_threshold=0,  # D√©sactiv√©
    alignment_repetition_threshold=0,  # D√©sactiv√©
    excessive_tail_threshold=0,  # D√©sactiv√©
):
    ...
```

### üéØ Cas 4 : Param√®tres √©quilibr√©s (recommand√©)

```python
for audio_chunk, metrics in model.generate_stream(
    text,
    # √âchantillonnage
    repetition_penalty=1.2,
    min_p=0.0,
    top_p=0.95,
    # D√©tection d'hallucination
    token_repetition_threshold=5,
    long_tail_threshold=5,
    alignment_repetition_threshold=5,
    excessive_tail_threshold=10,
):
    ...
```

## Exemples de code

Voir le fichier `example_advanced_streaming.py` pour des exemples complets montrant :
1. Param√®tres par d√©faut (√©quilibr√©s)
2. Toute d√©tection d√©sactiv√©e (g√©n√©ration maximale)
3. Param√®tres tr√®s stricts (pr√©venir les coupures pr√©matur√©es)
4. D√©tection tr√®s aggressive (arr√™t rapide)
5. D√©sactivation s√©lective (seulement la d√©tection de tokens)

## Ex√©cuter les exemples

```bash
# Exemple de base
uv run python example_tts_stream.py

# Exemple avanc√© avec tous les param√®tres
uv run python example_advanced_streaming.py
```

## Logs et d√©bogage

Le syst√®me log automatiquement les d√©tections :
```
üö® Detected 5x repetition of token 1234
forcing EOS token, long_tail=True, alignment_repetition=False, token_repetition=False, excessive_tail=False
```

Pour voir ces logs, activez le logging :
```python
import logging
logging.basicConfig(level=logging.WARNING)
```

## Migration depuis l'ancienne version

**Avant** (hardcod√©) :
```python
# Pas de contr√¥le sur la d√©tection
for audio_chunk, metrics in model.generate_stream(text):
    ...
```

**Maintenant** (configurable) :
```python
# Contr√¥le total
for audio_chunk, metrics in model.generate_stream(
    text,
    token_repetition_threshold=8,  # Personnalisable
    repetition_penalty=1.3,  # Personnalisable
):
    ...
```

## Contribution

Ces am√©liorations ont √©t√© ajout√©es pour r√©soudre les probl√®mes de :
1. D√©tection trop agressive de r√©p√©tition (arr√™t pr√©matur√©)
2. Hallucinations audio apr√®s la fin du texte
3. Manque de flexibilit√© dans les param√®tres d'√©chantillonnage

Pour plus d'informations, voir les fichiers modifi√©s :
- `src/chatterbox/tts.py` : Ajout des param√®tres dans `generate_stream()` et `inference_stream()`
- `src/chatterbox/models/t3/inference/alignment_stream_analyzer.py` : Seuils configurables
